{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alone-integer",
   "metadata": {},
   "source": [
    "# Improving a Machine Learning model\n",
    "\n",
    "* First predictions = **Baseline predictions.**\n",
    "* First model = **Baseline model.**\n",
    "\n",
    "From a **data** perspective:\n",
    "* Could we collect more data? \n",
    "* Could we improve our data?\n",
    "\n",
    "From a **model** perspective:\n",
    "* Is there a better model we could use? (see scikit-learn model flow chart)\n",
    "* Could we improve the current model?\n",
    "\n",
    "Hyperparameters vs Parameters\n",
    "* **Parameters** = The model finds these patterns in data.\n",
    "* **Hyperparameters** = Settings on a model, that you adjust to (potentially) improve its ability to find patterns.\n",
    "\n",
    "Three ways to adjust Hyperparameters:\n",
    "* By hand\n",
    "* Randomly with RandomSearchCV\n",
    "* Exhaustively with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "forward-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "worthy-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "boston_df = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "boston_df['target'] = pd.Series(boston['target'])\n",
    "\n",
    "\n",
    "heart_disease = pd.read_csv('~/sample_project/Data/heart-disease.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "varied-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific imports \n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-dimension",
   "metadata": {},
   "source": [
    "**Improving the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "sustainable-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fresh-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-accounting",
   "metadata": {},
   "source": [
    "### 5.1 Tuning hyperparameters by hand\n",
    "\n",
    "Make 3 sets: \n",
    "* Training.\n",
    "* Validation.\n",
    "* Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "existing-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-prairie",
   "metadata": {},
   "source": [
    "We will try to adjust the following:\n",
    "* `max-depth`.\n",
    "* `max_features`.\n",
    "* `min_samples_leaf`.\n",
    "* `min_samples_split`.\n",
    "* `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "future-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true vs. y_pred labels, on a classification model.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2),\n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-arena",
   "metadata": {},
   "source": [
    "#### Manually splittiing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-democracy",
   "metadata": {},
   "source": [
    "###### Shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "particular-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "heart_disease_shuffled = heart_disease.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-florence",
   "metadata": {},
   "source": [
    "###### Split into X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "underlying-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease_shuffled.drop('target', axis=1)\n",
    "y = heart_disease_shuffled['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-oxide",
   "metadata": {},
   "source": [
    "###### Split the data into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "demonstrated-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = round(0.7 * len(heart_disease_shuffled))\n",
    "validation_split = round(train_split + 0.15 * len(heart_disease_shuffled))\n",
    "\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "\n",
    "X_valid, y_valid = X[train_split:validation_split], y[train_split:validation_split]\n",
    "\n",
    "X_test, y_test = X[validation_split:], y[validation_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "destroyed-collect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 45, 46)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_valid), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "apparent-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-jungle",
   "metadata": {},
   "source": [
    "**baseline parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "outstanding-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "included-prior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.44%\n",
      "Precision: 0.85\n",
      "recall: 0.88\n",
      "F1 score: 0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.84, 'precision': 0.85, 'recall': 0.88, 'f1': 0.86}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make baseline predictions, on the valid set. As this is what we will be tuning the hyperparameters on.\n",
    "\n",
    "y_preds = clf.predict(X_valid)\n",
    "\n",
    "# Evaluate the classifier on valid set\n",
    "\n",
    "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-coffee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
